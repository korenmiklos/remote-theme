---
title: Coding for Economists
description: Use your computer effectively for reproducible research.
tags:
- top
categories:
- ipsum
author: john
---

Ok, this is a confusing title. Both “regression” and “testing” have a formal definition in statistics. And [“regression testing”](#) is a software engineering term for making sure that changes to your code did not introduce any unwanted change in its behavior.
As data scientists, we engage in regression testing all the time. Suppose I estimated that, in Hungarian manufacturing firms between 1992 and 2014, foreign managers improve firm productivity by 15 percent relative to domestic managers. Then the vendor sends an additional year’s worth of data. The first thing I want to check is how my estimate changes. Or we come up with a new algorithm to disambiguate manager names. How do the results change?

![Lorem](/assets/images/content.jpg)

Social distancing interventions can be effective against epidemics but are potentially detrimental for the economy. Businesses that rely heavily on face-to-face communication or close physical proximity when producing a product or providing a service are particularly vulnerable. There is, however, no systematic evidence about the role of human interactions across different lines of business and about which will be the most limited by social distancing. Here we provide theory-based measures of the reliance of U.S. businesses on human interaction, detailed by industry and geographic location. We find that, before the pandemic hit, 43 million workers worked in occupations that rely heavily on face-to-face communication or require close physical proximity to other workers. Many of these workers lost their jobs since. Consistently with our model, employment losses have been largest in sectors that rely heavily on customer contact and where these contacts dropped the most: retail, hotels and restaurants, arts and entertainment and schools. Our results can help quantify the economic costs of social distancing.

```
assert estimate1.coefficient.similar(estimate2.coefficient)
assert estimate1.coefficient.significant() == estimate2.coefficient.significant()
```

## Lorem ipsum dolorem

#### How can we make this testing more reproducible?

At the very least, we should document every step we did. I sometimes create new branches in my git repo with names like experiment/narrow-sample. These are often just a couple of commits in which I learn how my results would change if I used a narrower sample definition, for example. Then I go back to my master branch, leaving these short branches dangling. I leave a record of my tests, but I am not sure this is a proper use git branching.
We can also automate some of these tests. Cross validation in machine learning is one example of such automated testing. We can add various assertions in simple unit tests. For example, if two Stata commands can be used to estimate the same model, I can