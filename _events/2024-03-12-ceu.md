---
author: Miklós Koren (@korenmiklos)
speaker: Miklós Koren
title: Finding Things by Name
aspectratio: 1610
lang: en
titlepage: true
code: missing
description: Guest lecture
image: https://images.unsplash.com/photo-1500334997201-a5d21be0f328?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2ODAxOTV8MHwxfHJhbmRvbXx8fHx8fHx8fDE3MzI2NDM2MTZ8&ixlib=rb-4.0.3&q=80&w=1080
categories:
- teaching
- entity resolution
event_date: 2024-03-12
location: Vienna, Austria
links:
- text: Slides
  url: https://github.com/korenmiklos/talks/blob/master/2024-03-12-ceu/README.pdf
---

## Showcase problem
City names are often misspelled. Many cities with the same or very similar name. Cities are often mentioned in a different language.

## Algorithms and data structures
Find if a word is in `/usr/share/dict/words`. Loop through list. On average, do $N/2$ comparisons.
$$
O(KN)
$$
Introduce big-O notation.

Binary search gives $O(K \log N)$. Divide-and-conquer algorithms, trees.

Hash table gives $O(1)$. First letter of alphabet.

## Start with a whitelist
GeoNames.org has a list of cities. 2.5 million cities


## Reading
1. [Falsehoods about names](https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/)
2. [Beider-Morse phonetic matching](https://stevemorse.org/phonetics/bmpm2.htm)
3. Joel Spolsky's [essay on Unicode](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/)
4. Read a [short introduction](https://dev.to/korenmiklos/wish-i-could-be-like-david-watts-2edp) to the problem of entity resolution.



# Entity Resolution

## Appreciate uncertainty
The first guiding principle of entity resolution is to embrace the imperfections. There is no perfect method, you are just balancing two types of error. _False positives_ occur when you link two observations that, in reality, refer to two different entities. _False negatives_ occur when you fail to link two observations that, in reality, represent the same entity. You can always decrease one type of error at the expense of the other by selecting a more or less stringent matching method.

The accuracy of your method can be characterized by its _precision_ and _recall rates_.

![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png)

Typically the business costs of false positives are larger (you don't want to send a customer's account statement to someone else). But the costs of false negatives can be substantial too (users signing up twice for your freemium service).

|------|-----------|
| Name | Person ID |
|------|-----------|
| Viktor | 1 |
| Victor | 1 |
| Victoria | 2|
| Yevgeniy | 3 |
| Evgeni | 3 |

## Exercise
Compute the pairwise [Levenshtein distances](https://planetcalc.com/1721/) between _lowercase_ names. This counts the number of character edits you have to make for string A to exactly match string B. For example, "Vi**k**tor" is at 1 distance from "Vi**c**tor".

## Exercise
Suppose your ER algorithm accepts names as referring to the same entity if they are within Levenshtein distance _d_. What are the false positive and false negative counts for different values of _d_? What are the precision and recall rates?

You may have seen these type of errors in binary classification problems. Indeed, ER can be aided by supervised machine learning. Suppose we have a set of features _X_ in each record that are potentially informative about their entity. I need to decide, for each _X1_ and _X2_, whether they refer to the same entity, that is, whether _f(X1, X2)=1_. Given a training sample on _y = f(X1, X2)_, this is a standard machine learning problem.

### How do I get a training sample? 
1. I can hand label a training sample. This should not be a random sample, because matches/duplicates are rare. Because for a sample size _k_ I have to make O(k^2) comparisons, this is very costly.
2. For a subset of observation, I may have unique identifiers. For example, I have collected noy only names, but also student IDs in one year. 


## Appreciate complexity
The second guiding principle is to appreciate the computational complexity. If you are unsure about your data, you have to compare every observation with every other, making `N(N-1)/2` comparisons in a dataset with `N` observations. (See box on why it is sufficient to make _pairwise_ comparisons.) In a large dataset this becomes prohibitively many comparisons. For example, if you want to deduplicate users from a dataset with 100,000 observations (a small dataset), you have to make 10 _billion_ comparisons. Throughout the ER process, you should be looking for ways to reduce the number of necessary comparisons.

Comparing each pair of _N_ records is of complexity O(N^2). Avoid doing this.

How do we reduce complexity? Creating an index can help but it will take up a large space. For example, we can index all [n-grams](https://en.wikipedia.org/wiki/N-gram) appearing in the text.

|------|-----------|
| Name | 2-grams |
|------|-----------|
| Viktor | vi,ik,kt,to,or |
| Victor | vi,ic,ct,to,or |
| Victoria | vi,ic,ct,to,or,ri,ia |
| Yevgeniy | ye,ev,vg,ge,en,ni,iy |
| Evgeni | ev,vg,ge,en,ni |

Then we can look at only pairs that have at least one n-gram in common. This would search in a hash index so would not need any comparison. We would only search within groups sharing n-grams.

In the example above, "Viktor" would never be compared to "Yevgeny", only to "Victor" and "Victoria".

Alternatively, we can create an index based on the first letter (these are rarely mistyped). But then we would miss "Evgeni" = "Yevgeniy".

## Three types of ER
1. Deduplicate a list
2. Merge two lists
3. Check against a whitelist


## Four steps to ER
1. Normalize
2. Bin
3. Match
4. Merge

First you __normalize__ your data by eliminating typos, alternative spellings, to bring the data to a more structured, more comparable format. For example, a name "Dr David George Watts III" may be normalized to "watts, david." Normalization can give you a lot of efficiency because your comparisons in the next step will be much easier. However, this is also where you can loose the most information if you are over-normalizing. 

Normalization (a.k.a. standardization) is a function that maps your observation to a simpler (often text) representation. During a normalization, you only use one observation and do not compare it to any other observation. That comes later. You can compare to (short) _white lists_, though. For example, if your observations represent cities, it is useful to compare the `city_name` field to a list of known cities and correct typos. You can also convert text fields to lower case, drop punctuation and _stop words_, round or bin numerical values.

If there is a canonical way to represent the information in your observations, use that. For example, the US Postal Services standardizes US addresses (see figure) and [provides an API](https://www.usps.com/business/web-tools-apis/address-information-api.htm) to do that. 

![](https://thepracticaldev.s3.amazonaws.com/i/dy4d171gkjmql3lltxr4.png)

To minimize the number of comparisons, you typically only evaluate _potential matches_. This is where normalization can be helpful, as you only need to compare observations with normalized names of "watts, david," or those within the same city, for example. Decide on the __bins__ you are comparing within.

Then you __match__ pairs of observations which are close enough according to your metric. The metric can allow for typos, such as a _Levenshtein distance_. It can rely on multiple fields such as name, address, phone number, date of birth. You can assign weights to each of these fields: matching on phone number may carry a large weight than matching on name. You can also opt for a _decision tree_: only check the date of birth and phone number for very common names, for example.

Once you matched related observations, you have to __merge__ the information they provide about the entity they represent. For example, if you are matching "Dr David Watts" and "David Watts," you have to decide whether the person is indeed a "Dr" and whether you are keeping that information. The merge step involves aggregating information from the individual observations with whatever aggregation function you feel appropriate. You can fill in missing fields (if, say, you find the phone number for David Watts in one observation, use it throughout), use the most complete text representation (such as "Dr David George Watts III"), or simply keep all the variants of a field (by creating a _set_ of name variants, for example, {"David Watts", "Dr David Watts", "Dr David George Watts III"}).